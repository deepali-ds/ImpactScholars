{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###### We used the CDS API to download ERA5 reanalysis data by specifying key parameters such as temperature and dew point, along with the desired time range and spatial coverage. After registering on the Copernicus Climate Data Store (CDS) and configuring the API, we submitted a request to retrieve the data in NetCDF format. This allowed us to efficiently access high-resolution climate information, enabling a detailed analysis of temperature trends and extreme heat events over time. The automated download process streamlined data retrieval, making it easier to work with large climate datasets.######"
      ],
      "metadata": {
        "id": "4gHdLQL2iZJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTlnKuiSXCu7",
        "outputId": "95706871-bf46-4d03-c08d-218a6fe274a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install cdsapi geopandas rioxarray xarray pandas netCDF4"
      ],
      "metadata": {
        "id": "HbVzFrPMXBJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! pip install \"cdsapi>=0.7.2\""
      ],
      "metadata": {
        "id": "tyBIqCGkXLQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cdsapi\n",
        "import xarray as xr\n",
        "import geopandas as gpd\n",
        "import rioxarray"
      ],
      "metadata": {
        "id": "XPwdOR_3ZWMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting sample temperature 2m data for one year\n",
        "import cdsapi\n",
        "\n",
        "dataset = \"derived-era5-single-levels-daily-statistics\"\n",
        "request = {\n",
        "    \"product_type\": \"reanalysis\",\n",
        "    \"variable\": [\"2m_temperature\"],\n",
        "    \"year\": \"2001\",\n",
        "    \"month\": [\n",
        "        \"01\", \"02\", \"03\",\n",
        "        \"04\", \"05\", \"06\",\n",
        "        \"07\", \"08\", \"09\",\n",
        "        \"10\", \"11\", \"12\"\n",
        "    ],\n",
        "    \"day\": [\n",
        "        \"01\", \"02\", \"03\",\n",
        "        \"04\", \"05\", \"06\",\n",
        "        \"07\", \"08\", \"09\",\n",
        "        \"10\", \"11\", \"12\",\n",
        "        \"13\", \"14\", \"15\",\n",
        "        \"16\", \"17\", \"18\",\n",
        "        \"19\", \"20\", \"21\",\n",
        "        \"22\", \"23\", \"24\",\n",
        "        \"25\", \"26\", \"27\",\n",
        "        \"28\", \"29\", \"30\",\n",
        "        \"31\"\n",
        "    ],\n",
        "    \"daily_statistic\": \"daily_mean\",\n",
        "    \"time_zone\": \"utc+00:00\",\n",
        "    \"frequency\": \"6_hourly\",\n",
        "    \"area\": [35, 68, 6, 97]\n",
        "}\n",
        "\n",
        "client = cdsapi.Client()\n",
        "client.retrieve(dataset, request).download()"
      ],
      "metadata": {
        "id": "UglkXjLbXOVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy plotly\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BXZWZEbZqUg",
        "outputId": "ac57d4d0-65f7-489e-919a-4cd11c2fe452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "import plotly.express as px\n",
        "from scipy.stats import pearsonr\n",
        "import glob\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from scipy import stats\n"
      ],
      "metadata": {
        "id": "RRU2b62yW52S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ERA_5_INDIA_DATA_DIR = \"/content/drive/MyDrive/ERA_5_INDIA_DATA_DIR\""
      ],
      "metadata": {
        "id": "QTDreTkp57Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Computes relative humidity (RH) from temperature (t) and dewpoint temperature (d).\n",
        "#Uses an empirical equation to estimate Wet Bulb Temperature (WBT).\n",
        "#Returns WBT.\n",
        "\n",
        "def calculate_wbt(t, d):\n",
        "    \"\"\"Approximate Wet Bulb Temperature (WBT) from temperature and dewpoint.\"\"\"\n",
        "    rh = 100 * (np.exp((17.625 * d) / (243.04 + d)) / np.exp((17.625 * t) / (243.04 + t)))  # Relative Humidity\n",
        "    wbt = t * np.arctan(0.151977 * (rh + 8.313659)**0.5) + \\\n",
        "          np.arctan(t + rh) - np.arctan(rh - 1.676331) + \\\n",
        "          0.00391838 * rh**1.5 * np.arctan(0.023101 * rh) - 4.686035\n",
        "    return wbt"
      ],
      "metadata": {
        "id": "0XdM9XKD6DSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reads a GeoJSON file containing state boundaries for India.\n",
        "# This is used to map WBT values to specific states.\n",
        "# Iterates through years (2000 to 2023).\n",
        "# For each year, iterates through months (Jan-Dec).\n",
        "# Processes WBT data monthly.\n",
        "# Defines file paths for temperature (t2m) and dewpoint temperature (d2m) for a given month.\n",
        "# Reads these NetCDF files using xarray\n",
        "# ERA5 temperature data is in Kelvin, so we subtract 273.15 to convert to Celsius.\n",
        "# Uses the calculate_wbt function to compute WBT for each grid point in the dataset.\n",
        "# Converts xarray dataset into a pandas DataFrame.\n",
        "# Renames columns for clarity.\n",
        "# Creates geospatial points for each data location using longitude and latitude.\n",
        "# Converts DataFrame to GeoDataFrame for spatial operations.\n",
        "# Performs a spatial join to assign each WBT data point to the corresponding state.\n",
        "# Extracts year, month, and day from the time column for grouping and analysis.\n",
        "# Groups by year, month, day, and state (st_nm).\n",
        "\n",
        "# Computes mean values for temperature, dewpoint, and WBT.\n",
        "# Concatenates monthly WBT data into a single yearly DataFrame.\n",
        "# Saves the results to a CSV file.\n",
        "# Collects all yearly CSV files.\n",
        "# Reads them into a list of DataFrames.\n",
        "# Merges all years into a single CSV file.\n",
        "\n",
        "\n",
        "\n",
        "# Load shapefile from GeoJSON\n",
        "states_gdf = gpd.read_file(\"/content/drive/MyDrive/Indian_states/Indian_states.geojson\")\n",
        "\n",
        "for year in range(2000, 2024):\n",
        "    year_wbt = []\n",
        "    for month in range(1, 13):\n",
        "        t2m_file_path = f\"{ERA_5_INDIA_DATA_DIR}/max-1-hourly/{year}-{month}/2m_temperature_0_daily-max.nc\"\n",
        "        d2m_file_path = f\"{ERA_5_INDIA_DATA_DIR}/max-1-hourly/{year}-{month}/2m_dewpoint_temperature_stream-oper_daily-max.nc\"\n",
        "\n",
        "        print(f\"Processing {t2m_file_path}, {d2m_file_path}\")\n",
        "\n",
        "        # Load the netCDF files\n",
        "        ds = xr.open_dataset(t2m_file_path)\n",
        "        ds_d2m = xr.open_dataset(d2m_file_path)\n",
        "\n",
        "        ds['t2m'] = ds['t2m'] - 273.15  # Convert to Celsius\n",
        "        ds['d2m'] = ds_d2m['d2m'] - 273.15  # Convert to Celsius\n",
        "\n",
        "        # Rename valid_time to time (if needed)\n",
        "        if 'valid_time' in ds:\n",
        "            ds = ds.rename({'valid_time': 'time'})\n",
        "\n",
        "        # Apply WBT calculation\n",
        "        ds['wbt'] = calculate_wbt(ds['t2m'], ds['d2m'])\n",
        "\n",
        "        df = ds[['t2m', 'd2m', 'wbt']].to_dataframe().reset_index()\n",
        "        df = df.rename(columns={'t2m': 'Temperature_C', 'd2m': 'Dewpoint_C', 'wbt': 'Wet_Bulb_Temperature_C'})\n",
        "\n",
        "        # Convert DataFrame grid points to GeoDataFrame\n",
        "        geometry = [Point(lon, lat) for lon, lat in zip(df['longitude'], df['latitude'])]\n",
        "        points_gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
        "\n",
        "        # Perform spatial join to map points to states\n",
        "        points_with_states = gpd.sjoin(points_gdf, states_gdf, how=\"inner\", predicate=\"intersects\")\n",
        "\n",
        "        # Extract year, month and day from 'time' if it is a datetime object\n",
        "        points_with_states['year'] = points_with_states['time'].dt.year\n",
        "        points_with_states['month'] = points_with_states['time'].dt.month\n",
        "        points_with_states['day'] = points_with_states['time'].dt.day\n",
        "\n",
        "        # Group by year and state (using 'st_nm' for state names)\n",
        "        year_wbt.append(points_with_states.groupby(['year', 'month', 'day', 'st_nm'])[['Temperature_C', 'Dewpoint_C', 'Wet_Bulb_Temperature_C']].mean().reset_index())\n",
        "\n",
        "    # Concatenate all_wbt and save as a CSV file\n",
        "    year_wbt_df = pd.concat(year_wbt, ignore_index=True)\n",
        "    year_wbt_df.to_csv(f\"{ERA_5_INDIA_DATA_DIR}/statewise_wbt_max_{year}_test.csv\", index=False)"
      ],
      "metadata": {
        "id": "Uu_JYV906DJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file path\n",
        "# This CSV contains daily mean max temperature, dewpoint, and WBT for each state.\n",
        "\n",
        "csv_files = glob.glob(\"/content/drive/MyDrive/ERA_5_INDIA_DATA_DIR/statewise_wbt_max_*_test.csv\")\n",
        "\n",
        "# Read and concatenate all CSVs\n",
        "df_list = [pd.read_csv(file) for file in csv_files]\n",
        "df_all = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# This final CSV contains daily records for every state over all years.\n",
        "# Save the merged CSV\n",
        "df_all.to_csv(\"/content/drive/MyDrive/ERA_5_INDIA_DATA_DIR/statewise_wbt_max_all_years.csv\", index=False)\n",
        "\n",
        "print(\"Merged CSV file saved successfully!\")\n",
        "\n",
        "# Each daily record is statewise, meaning temperature and dew point values are aggregated across grid points within each state.\n",
        "# The final dataset provides a statewise daily time series from 2000 to 2023.\n",
        "# All years are merged into a single statewise daily dataset.\n",
        "\n",
        "\"\"\"Each daily record is statewise, meaning that:\n",
        "\n",
        "    Temperature (t2m) values are aggregated across all grid points within each state.\n",
        "    Dewpoint (d2m) values are also aggregated across all grid points within each state.\n",
        "    Wet Bulb Temperature (WBT) is computed for each grid point, then aggregated statewise.\n",
        "\n",
        "So for every state and day, the dataset contains:\n",
        " Mean temperature (°C) across the state\n",
        " Mean dewpoint (°C) across the state\n",
        " Mean wet bulb temperature (°C) across the state\n",
        "\n",
        "These are saved daily in the statewise CSV files. \"\"\""
      ],
      "metadata": {
        "id": "J9idKNiMmFkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Calculate heatwave days"
      ],
      "metadata": {
        "id": "7npVnj7JneaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate heatwave days based on the criteia; If WBT exceeds 90th percentile or above for 3 consecutive days, then its considered as heatwave event.\n",
        "\n",
        "# Ensure year, month, and day are properly converted to integers\n",
        "for col in ['year', 'month', 'day']:\n",
        "    df_all[col] = pd.to_numeric(df_all[col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "# Create the 'date' column\n",
        "df_all['date'] = pd.to_datetime(df_all[['year', 'month', 'day']], errors='coerce')\n",
        "\n",
        "# Ensure 'st_nm' is a string\n",
        "df_all['st_nm'] = df_all['st_nm'].astype(str).fillna(\"Unknown_State\")\n",
        "\n",
        "# Ensure Wet_Bulb_Temperature_C is numeric\n",
        "df_all['Wet_Bulb_Temperature_C'] = pd.to_numeric(df_all['Wet_Bulb_Temperature_C'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing temperature values\n",
        "df_all = df_all.dropna(subset=['Wet_Bulb_Temperature_C'])\n",
        "\n",
        "# Calculate the 90th percentile of Wet Bulb Temperature per state\n",
        "percentile_90 = df_all.groupby('st_nm')['Wet_Bulb_Temperature_C'].quantile(0.90).rename(\"wbt_90th_percentile\")\n",
        "\n",
        "# Check if percentile_90 contains valid data\n",
        "print(\"90th Percentile Data Sample:\\n\", percentile_90.head())\n",
        "\n",
        "# Drop existing column before merging to avoid duplication issues\n",
        "if \"wbt_90th_percentile\" in df_all.columns:\n",
        "    df_all = df_all.drop(columns=[\"wbt_90th_percentile\"])\n",
        "\n",
        "# Merge the computed percentiles into df_all\n",
        "df_all = df_all.merge(percentile_90, on=\"st_nm\", how=\"left\")\n",
        "\n",
        "# Verify if 'wbt_90th_percentile' exists in df_all\n",
        "print(\"Columns in df_all after merging:\", df_all.columns)\n",
        "\n",
        "# Flag heatwave days where WBT exceeds the 90th percentile\n",
        "df_all['heatwave_flag'] = df_all['Wet_Bulb_Temperature_C'] >= df_all['wbt_90th_percentile']\n",
        "df_all['heatwave_flag'] = df_all['heatwave_flag'].astype(int)  # Convert Boolean to int\n",
        "\n",
        "# Sort by state and date before processing streaks\n",
        "df_all = df_all.sort_values(by=['st_nm', 'date']).reset_index(drop=True)\n",
        "\n",
        "# Identify consecutive heatwave streaks (at least 3 days)\n",
        "df_all['streak'] = df_all.groupby('st_nm')['heatwave_flag'].apply(\n",
        "    lambda x: x * (x.groupby((x == 0).cumsum()).cumcount() + 1)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# Flag heatwave events where streak is 3 or more consecutive days\n",
        "df_all['heatwave_event'] = df_all['streak'] >= 3\n",
        "\n",
        "# Count heatwave days per state and year\n",
        "heatwave_stats = df_all[df_all['heatwave_event']].groupby(['year', 'st_nm']).agg(\n",
        "    heatwave_days=('heatwave_event', 'sum'),\n",
        "    median_wbt=('Wet_Bulb_Temperature_C', 'median')\n",
        ").reset_index()\n",
        "\n",
        "# Save the results\n",
        "output_path = \"/content/drive/MyDrive/ERA_5_INDIA_DATA_DIR/heatwave_summary.csv\"\n",
        "heatwave_stats.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "zxzP0m4wnN4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}